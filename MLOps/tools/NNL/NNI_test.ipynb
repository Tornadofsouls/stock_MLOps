{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py380\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "'''\n",
    "training_data = pd.read_csv('C:/Users/is_li/Desktop/paper/github/stock on MLOps/MLOps/tools/DVC/example/data/train.csv',\n",
    "index_col= [0,1],header=[0,1])\n",
    "valid_data = pd.read_csv('C:/Users/is_li/Desktop/paper/github/stock on MLOps/MLOps/tools/DVC/example/data/valid.csv',\n",
    "index_col= [0,1],header=[0,1])\n",
    "test_data = pd.read_csv('C:/Users/is_li/Desktop/paper/github/stock on MLOps/MLOps/tools/DVC/example/data/test.csv',\n",
    "index_col= [0,1],header=[0,1])\n",
    "'''\n",
    "df_data  = pd.read_csv('C:/Users/is_li/Desktop/paper/github/stock on MLOps/MLOps/tools/DVC/example/data/dataset.csv',\n",
    "index_col= [0,1],header=[0,1])\n",
    "df_data = df_data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional\t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建两个列表，用来存储数据的特征和标签\n",
    "data_feat, data_target = [],[]\n",
    "\n",
    "# 设每条数据序列有20组数据\n",
    "seq = 10\n",
    "feature_num = 10\n",
    "\n",
    "for index in range(len(df_data) - seq):\n",
    "    # 构建特征集\n",
    "    data_feat.append(df_data['feature'].iloc[:,range(feature_num)][index: index + seq].values)\n",
    "    # 构建target集\n",
    "    data_target.append(df_data['label'][index:index + seq])\n",
    "\n",
    "# 将特征集和标签集整理成numpy数组\n",
    "data_feat = np.array(data_feat)\n",
    "data_target = np.array(data_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611\n",
      "5489\n"
     ]
    }
   ],
   "source": [
    "# 这里按照8:2的比例划分训练集和测试集\n",
    "test_set_size = int(np.round(0.1*df_data.shape[0]))  # np.round(1)是四舍五入，\n",
    "train_size = data_feat.shape[0] - (test_set_size) \n",
    "print(test_set_size)  # 输出测试集大小\n",
    "print(train_size)     # 输出训练集大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape =  torch.Size([5489, 10, 10])\n",
      "y_train.shape =  torch.Size([5489, 10, 1])\n",
      "x_test.shape =  torch.Size([611, 10, 10])\n",
      "y_test.shape =  torch.Size([611, 10, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 这里第一个维度自动确定，我们认为其为batch_size，因为在LSTM类的定义中，设置了batch_first=True\n",
    "trainX = torch.from_numpy(data_feat[:train_size].reshape(-1,seq,feature_num)).type(torch.Tensor)   \n",
    "testX  = torch.from_numpy(data_feat[train_size:].reshape(-1,seq,feature_num)).type(torch.Tensor)\n",
    "trainY = torch.from_numpy(data_target[:train_size].reshape(-1,seq,1)).type(torch.Tensor)\n",
    "testY  = torch.from_numpy(data_target[train_size:].reshape(-1,seq,1)).type(torch.Tensor)\n",
    "print('x_train.shape = ',trainX.shape)\n",
    "print('y_train.shape = ',trainY.shape)\n",
    "print('x_test.shape = ',testX.shape)\n",
    "print('y_test.shape = ',testY.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train = torch.utils.data.TensorDataset(trainX,trainY)\n",
    "test = torch.utils.data.TensorDataset(testX,testY)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_dim = feature_num    # 数据的特征数\n",
    "hidden_dim = 2    # 隐藏层的神经元个数\n",
    "num_layers = 2     # LSTM的层数\n",
    "output_dim = 1     # 预测值的特征数\n",
    "                   #（这是预测股票价格，所以这里特征数是1，如果预测一个单词，那么这里是one-hot向量的编码长度）\n",
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size=1, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    def forward(self, _x):\n",
    "        x, _ = self.lstm(_x)  # _x is input, size (seq_len, batch, input_size)\n",
    "        s, b, h = x.shape  # x is output, size (seq_len, batch, hidden_size)\n",
    "        x = x.view(s * b, h)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(s, b, -1)  # 把形状改回来\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNI 超参数优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need pip install nni\n",
    "import nni\n",
    "from nni.utils import merge_parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\py380\\lib\\site-packages\\nni\\runtime\\platform\\standalone.py:32: RuntimeWarning: \u001b[1m\u001b[31mRunning trial code without runtime. Please check the tutorial if you are new to NNI: \u001b[33mhttps://nni.readthedocs.io/en/stable/tutorials/hpo_quickstart_pytorch/main.html\u001b[0m\n",
      "  warnings.warn(warning_message, RuntimeWarning)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'hidden_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 更新模型参数\u001b[39;00m\n\u001b[0;32m     10\u001b[0m params \u001b[38;5;241m=\u001b[39m nni\u001b[38;5;241m.\u001b[39mget_next_parameter()\n\u001b[1;32m---> 11\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     12\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     13\u001b[0m lr \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hidden_dim'"
     ]
    }
   ],
   "source": [
    "# NNI\n",
    "# 定义搜索空间\n",
    "params = {\n",
    "    \"hidden_dim\": nni.choice(2, 4, 8, 16),\n",
    "    \"num_layers\": nni.choice(1, 2, 3),\n",
    "    \"lr\": nni.loguniform(1e-5, 1e-1)\n",
    "}\n",
    "\n",
    "# 更新模型参数\n",
    "params = nni.get_next_parameter()\n",
    "hidden_dim = params['hidden_dim']\n",
    "num_layers = params['num_layers']\n",
    "lr = params['lr']\n",
    "\n",
    "\n",
    "# 定义模型\n",
    "num_epochs = 50\n",
    "model = LSTM(input_size=input_dim, hidden_size=hidden_dim, output_size=output_dim, num_layers=num_layers)\n",
    "# print(model)\n",
    "# 打印模型各层的参数尺寸\n",
    "\n",
    "for i in range(len(list(model.parameters()))):\n",
    "    print(list(model.parameters())[i].size())\n",
    "    \n",
    "loss_function = nn.MSELoss()  # 损失函数\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # 优化器\n",
    "\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "train_loss = [] \n",
    "for epoch in range(num_epochs):\n",
    "    out = model(trainX)\n",
    "    loss = loss_function(out, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.append(loss.item())\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch: {}, Loss:{:.10f}'.format(epoch + 1, loss.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出NNI最终结果\n",
    "nni.report_final_result(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制loss函数图像\n",
    "plt.plot(train_loss)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2287c915b3db61027d50cb6eb72a988b8702097dccd2eb0ffd81756531a8644e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py380')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
